/*  SPDX-License-Identifier: BSD-3-Clause
 *  Copyright(c) 2018 Marvell International Ltd.
 */

#include <rte_ethdev_driver.h>
#include <rte_kvargs.h>
#include <rte_log.h>
#include <rte_malloc.h>
#include <rte_bus_vdev.h>
#include <rte_net.h>
#include <fcntl.h>
#include <linux/ethtool.h>
#include <linux/sockios.h>
#include <net/if.h>
#include <net/if_arp.h>
#include <sys/ioctl.h>
#include <sys/socket.h>
#include <sys/stat.h>
#include <sys/types.h>

#include <rte_mvep_common.h>
#include "mvgiu_ethdev.h"

/* prefetch shift */
#define MRVL_MUSDK_PREFETCH_SHIFT 2
#define MRVL_IFACE_NAME_ARG "iface"
#define MRVL_CFG_ARG "cfg"
#define MRVL_COOKIE_ADDR_INVALID ~0ULL
#define MRVL_COOKIE_HIGH_ADDR_MASK (0xffffff0000000000)

#define MRVL_BURST_SIZE 64

/** Port Rx offload capabilities */
#define MVGIU_RX_OFFLOADS (DEV_RX_OFFLOAD_CHECKSUM)

/** Port Tx offloads capabilities */
#define MVGIU_TX_OFFLOADS (0)

static const char * const valid_args[] = {
	MRVL_IFACE_NAME_ARG,
	MRVL_CFG_ARG,
	NULL
};

static int mrvl_dev_num;

static int rte_pmd_mvgiu_remove(struct rte_vdev_device *vdev);

extern int rte_mvep_get_nmp_guest_info(struct nmp_guest_info *inf,
				       char **prb_str);
static uint16_t
mvgiu_rx_pkt_burst(void *rxq, struct rte_mbuf **rx_pkts, uint16_t nb_descs);
static uint16_t
mvgiu_tx_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts);
static uint16_t
mvgiu_tx_sg_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts);

static uint64_t cookie_addr_high = MRVL_COOKIE_ADDR_INVALID;

struct mvgiu_ifnames {
	const char *names[10];
	int idx;
};

/*
 * To use buffer harvesting based on loopback port shadow queue structure
 * was introduced for buffers information bookkeeping.
 *
 * Before sending the packet, related buffer information (pp2_buff_inf) is
 * stored in shadow queue. After packet is transmitted no longer used
 * packet buffer is released back to it's original hardware pool,
 * on condition it originated from interface.
 * In case it  was generated by application itself i.e: mbuf->port field is
 * 0xff then its released to software mempool.
 */
struct mvgiu_shadow_txq {
	u16 head;           /* write index - used when sending buffers */
	u16 tail;           /* read index - used when releasing buffers */
	u16 size;           /* queue occupied size */
	u16 num_to_release; /* number of buffers sent, that can be released */
	u16 total_size;     /* the shadow-q total size */
	u16 res;
	struct giu_buff_inf	*ent;
	struct giu_gpio_desc	*descs;
};

struct mvgiu_rxq {
	struct mvgiu_priv *priv;
	struct rte_mempool *mp;
	uint32_t size;
	int queue_id;
	int port_id;
	int cksum_enabled;
	uint64_t bytes_recv;
	uint64_t packets_recv;
	u16 data_offset; /* Offset of the data within the buffer */
};

struct mvgiu_txq {
	struct mvgiu_priv *priv;
	int queue_id;
	int port_id;
	uint64_t bytes_sent;
	uint64_t packets_sent;
	struct mvgiu_shadow_txq shadow_txq;
	int tx_deferred_start;
	uint32_t size;
};

/**
 * Release already sent buffers to bpool (buffer-pool).
 *
 * @param sq
 *   Pointer to the shadow queue.
 */
static inline void
mvgiu_free_sent_buffers(struct mvgiu_shadow_txq *sq)
{
	struct giu_buff_inf *entry;
	struct rte_mbuf *mbuf;
	uint16_t nb_done = 0;
	int i;
	int tail = sq->tail;

	nb_done = sq->num_to_release;
	sq->num_to_release = 0;

	for (i = 0; i < nb_done; i++) {
		entry = &sq->ent[tail];
		if (unlikely(!entry->addr)) {
			RTE_LOG(ERR, PMD,
				"Shadow memory @%d: cookie(%lx), pa(%lx)!\n",
				tail, (u64)entry->cookie,
				(u64)entry->addr);
			tail = (tail + 1) & (sq->total_size - 1);
			continue;
		}

		mbuf = (struct rte_mbuf *)entry->cookie;
		/*
		 * In case of S/G, only the first mbuf freed, the other segments
		 * skipped
		 */
		tail = (tail + mbuf->nb_segs) & (sq->total_size - 1);
		i += (mbuf->nb_segs - 1); /* +1 will be done by the 'for' */
		 /* Need to retrieve nb_segs, so free afterwards */
		rte_pktmbuf_free(mbuf);
		entry->cookie = 0;
		entry->addr = 0;
	}

	sq->tail = tail;
	sq->size -= nb_done;
}

/**
 * Release already sent buffers to bpool (buffer-pool).
 *
 * @param gpio
 *   Pointer to the port structure.
 * @param hif
 *   Pointer to the MUSDK hardware interface.
 * @param sq
 *   Pointer to the shadow queue.
 * @param tc
 *   Tc number.
 * @param qid
 *   Queue id number.
 */
static inline
void mvgiu_check_n_free_sent_buffers(struct giu_gpio *gpio,
				     struct mvgiu_shadow_txq *sq,
				     u8 tc,
				     u8 qid)
{
	u16 num_conf = 0;

	giu_gpio_get_num_outq_done(gpio, tc, qid, &num_conf);

	sq->num_to_release += num_conf;

	if (likely(sq->num_to_release < MVGIU_BUF_RELEASE_BURST_SIZE))
		return;

	mvgiu_free_sent_buffers(sq);
}

/**
 * Release buffers to hardware bpool (buffer-pool)
 *
 * @param rxq
 *   Receive queue pointer.
 * @param num
 *   Number of buffers to release to bpool.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int mvgiu_fill_bpool(struct mvgiu_rxq *rxq, uint16_t num)
{
	struct giu_buff_inf entries[num];
	struct giu_bpool *bpool;
	struct rte_mbuf *mbufs[num];
	unsigned int core_id;
	uint16_t i;
	int ret = 0;

	core_id = rte_lcore_id();
	if (core_id == LCORE_ID_ANY)
		core_id = 0;

	bpool = rxq->priv->bpool;

	ret = rte_pktmbuf_alloc_bulk(rxq->mp, mbufs, num);
	if (ret)
		return ret;

	if (cookie_addr_high == MRVL_COOKIE_ADDR_INVALID)
		cookie_addr_high =
			(uint64_t)mbufs[0] & MRVL_COOKIE_HIGH_ADDR_MASK;

	for (i = 0; i < num; i++) {
		if (((uint64_t)mbufs[i] & MRVL_COOKIE_HIGH_ADDR_MASK)
			!= cookie_addr_high) {
			RTE_LOG(ERR, PMD,
				"mbuf virtual addr high is out of range "
				"0x%x instead of 0x%x\n",
				(uint32_t)((uint64_t)mbufs[i] >> 32),
				(uint32_t)(cookie_addr_high >> 32));
			ret = -1;
			goto out;
		}

		entries[i].addr =
			rte_mbuf_data_iova_default(mbufs[i]);
		entries[i].cookie = (uintptr_t)mbufs[i];
	}

	giu_bpool_put_buffs(bpool, entries, &i);

out:
	for (; i < num; i++)
		rte_pktmbuf_free(mbufs[i]);

	return ret;
}

/**
 * Configure RX Queues in a given port.
 *
 * Sets up RX queues, their Traffic Classes and DPDK rxq->(TC,inq) mapping.
 *
 * @param priv Port's private data
 * @param max_queues Maximum number of queues to configure.
 * @returns 0 in case of success, negative value otherwise.
 */
static int
mvgiu_configure_rxqs(struct mvgiu_priv *priv, uint16_t max_queues)
{
	size_t i;

	/* Direct mapping of queues i.e. 0->0, 1->1 etc. */
	for (i = 0; i < max_queues; ++i) {
		priv->rxq_map[i].tc = 0;
		priv->rxq_map[i].inq = i;
	}

	return 0;
}

/**
 * Ethernet device configuration.
 *
 * Prepare the driver for a given number of TX and RX queues and
 * configure RSS.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_dev_configure(struct rte_eth_dev *dev)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	int ret;

	if (dev->data->dev_conf.rxmode.mq_mode != ETH_MQ_RX_NONE &&
	    dev->data->dev_conf.rxmode.mq_mode != ETH_MQ_RX_RSS) {
		RTE_LOG(INFO, PMD, "Unsupported rx multi queue mode %d\n",
			dev->data->dev_conf.rxmode.mq_mode);
		return -EINVAL;
	}

	if (dev->data->dev_conf.rxmode.offloads & DEV_RX_OFFLOAD_KEEP_CRC) {
		RTE_LOG(INFO, PMD, "L2 CRC keeping not supported\n");
		return -EINVAL;
	}

	if (dev->data->dev_conf.rxmode.offloads & DEV_RX_OFFLOAD_VLAN_STRIP) {
		RTE_LOG(INFO, PMD, "VLAN stripping not supported\n");
		return -EINVAL;
	}

	if (dev->data->dev_conf.rxmode.split_hdr_size) {
		RTE_LOG(INFO, PMD, "Split headers not supported\n");
		return -EINVAL;
	}

	if (priv->gpio_capa.sg_en &&
	    !(dev->data->dev_conf.rxmode.offloads & DEV_RX_OFFLOAD_SCATTER)) {
		RTE_LOG(INFO, PMD, "RX Scatter MUST be set\n");
		return -EINVAL;
	}

	if (dev->data->dev_conf.rxmode.offloads & DEV_RX_OFFLOAD_TCP_LRO) {
		RTE_LOG(INFO, PMD, "LRO not supported\n");
		return -EINVAL;
	}

	if (dev->data->dev_conf.rxmode.offloads & DEV_RX_OFFLOAD_JUMBO_FRAME)
		dev->data->mtu = dev->data->dev_conf.rxmode.max_rx_pkt_len -
				 RTE_ETHER_HDR_LEN - RTE_ETHER_CRC_LEN;

	dev->rx_pkt_burst = mvgiu_rx_pkt_burst;
	dev->tx_pkt_burst = mvgiu_tx_pkt_burst;
	/* Tx -Scatter/Gather */
	if (dev->data->dev_conf.txmode.offloads & DEV_TX_OFFLOAD_MULTI_SEGS) {
		RTE_LOG(INFO, PMD, "Using multi-segment tx callback\n");
		dev->tx_pkt_burst = mvgiu_tx_sg_pkt_burst;
	}

	ret = mvgiu_configure_rxqs(priv, dev->data->nb_rx_queues);
	if (ret < 0)
		return ret;

	priv->nb_rx_queues = dev->data->nb_rx_queues;

	/* MVGIU is always in promiscuous mode */
	dev->data->promiscuous = 1;

	return 0;
}

/**
 * DPDK callback to bring the link up.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_dev_set_link_up(struct rte_eth_dev *dev)
{
	struct mvgiu_priv *priv = dev->data->dev_private;

	return giu_gpio_enable(priv->gpio);
}

/**
 * DPDK callback to bring the link down.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_dev_set_link_down(struct rte_eth_dev *dev)
{
	struct mvgiu_priv *priv = dev->data->dev_private;

	return giu_gpio_disable(priv->gpio);
}

/**
 * DPDK callback to start the device.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 *
 * @return
 *   0 on success, negative errno value on failure.
 */
static int
mvgiu_dev_start(struct rte_eth_dev *dev)
{
	return mvgiu_dev_set_link_up(dev);
}

/**
 * DPDK callback to stop the device.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_dev_stop(struct rte_eth_dev *dev)
{
	mvgiu_dev_set_link_down(dev);
}

/**
 * DPDK callback to retrieve physical link information.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param wait_to_complete
 *   Wait for request completion (ignored).
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_link_update(struct rte_eth_dev *dev,
		  int wait_to_complete __rte_unused)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	int link_up;

	dev->data->dev_link.link_speed = ETH_SPEED_NUM_10G;
	dev->data->dev_link.link_duplex = ETH_LINK_FULL_DUPLEX;
	dev->data->dev_link.link_autoneg = ETH_LINK_FIXED;

	giu_gpio_get_link_state(priv->gpio, &link_up);
	dev->data->dev_link.link_status = link_up ? ETH_LINK_UP : ETH_LINK_DOWN;

	return 0;
}

/**
 * Flush receive queues.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_flush_rx_queues(struct rte_eth_dev *dev)
{
	struct rte_mbuf *mbuf;
	uint64_t addr;
	int i, j, ret;

	RTE_LOG(INFO, PMD, "Flushing rx queues\n");
	for (i = 0; i < dev->data->nb_rx_queues; i++) {
		uint16_t num;

		do {
			struct mvgiu_rxq *q = dev->data->rx_queues[i];
			struct giu_gpio_desc descs[MRVL_BURST_SIZE];

			num = MRVL_BURST_SIZE;
			ret = giu_gpio_recv(q->priv->gpio,
					    q->priv->rxq_map[q->queue_id].tc,
					    q->priv->rxq_map[q->queue_id].inq,
					    descs, &num);
			for (j = 0; j < num; j++) {
				addr = giu_gpio_inq_desc_get_cookie(&descs[j]);
				mbuf = (struct rte_mbuf *)
					(cookie_addr_high | addr);
				rte_pktmbuf_free(mbuf);
			}
		} while (ret == 0 && num);
	}
}

/**
 * Flush transmit shadow queues.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_flush_tx_shadow_queues(struct rte_eth_dev *dev)
{
	struct mvgiu_txq *txq;
	struct mvgiu_shadow_txq *sq;
	struct giu_buff_inf *entry;
	struct rte_mbuf *mbuf;
	int i;

	RTE_LOG(INFO, PMD, "Flushing tx shadow queues\n");
	for (i = 0; i < dev->data->nb_tx_queues; i++) {
		txq = (struct mvgiu_txq *)dev->data->tx_queues[i];
		sq = &txq->shadow_txq;
		sq->num_to_release = sq->size;
		mvgiu_free_sent_buffers(sq);
		while (sq->tail != sq->head) {
			entry = &sq->ent[sq->tail];
			if (unlikely(!entry->addr)) {
				sq->tail = (sq->tail + 1) &
					(sq->total_size - 1);
				continue;
			}

			mbuf = (struct rte_mbuf *)entry->cookie;
			rte_pktmbuf_free(mbuf);
			sq->tail = (sq->tail + 1) &
				    (sq->total_size - 1);
		}
		memset(sq, 0, sizeof(*sq));
	}
}

/**
 * Flush hardware bpool (buffer-pool).
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_drain_bpool(struct mvgiu_priv *priv __rte_unused,
		  uint32_t num __rte_unused)
{
	/* TODO - there is no API to get buffers from the pool, so we need to
	 * record all buffers in a local queue
	 */
}

/**
 * Flush hardware bpool (buffer-pool).
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_flush_bpool(struct rte_eth_dev *dev)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	uint32_t num;
	int ret;

	ret = giu_bpool_get_num_buffs(priv->bpool, &num);
	if (ret) {
		RTE_LOG(ERR, PMD, "Failed to get bpool buffers number\n");
		return;
	}

	mvgiu_drain_bpool(priv, num);
}

/**
 * DPDK callback to close the device.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 */
static void
mvgiu_dev_close(struct rte_eth_dev *dev)
{
	struct mvgiu_priv *priv;

	mvgiu_flush_rx_queues(dev);
	mvgiu_flush_tx_shadow_queues(dev);
	mvgiu_flush_bpool(dev);

	priv = dev->data->dev_private;
	if (priv->gpio)
		giu_gpio_remove(priv->gpio);
	if (priv->bpool)
		giu_bpool_remove(priv->bpool);

	mrvl_dev_num--;

	if (mrvl_dev_num == 0)
		rte_mvep_deinit(MVEP_MOD_T_GIU);
}

/**
 * DPDK callback to get information about the device.
 *
 * @param dev
 *   Pointer to Ethernet device structure (unused).
 * @param info
 *   Info structure output buffer.
 */
static int
mvgiu_dev_infos_get(struct rte_eth_dev *dev,
		   struct rte_eth_dev_info *info)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	int i;

	info->speed_capa = ETH_LINK_SPEED_10M |
			   ETH_LINK_SPEED_100M |
			   ETH_LINK_SPEED_1G |
			   ETH_LINK_SPEED_10G;

	for (i = 0; i < priv->gpio_capa.intcs_inf.num_intcs; i++) {
		struct giu_gpio_intc_info *intcs_inf =
			&priv->gpio_capa.intcs_inf.intcs_inf[i];
		info->max_rx_queues += intcs_inf->num_inqs;
	}

	for (i = 0; i < priv->gpio_capa.outtcs_inf.num_outtcs; i++) {
		struct giu_gpio_outtc_info *outtcs_inf =
			&priv->gpio_capa.outtcs_inf.outtcs_inf[i];
		info->max_tx_queues += outtcs_inf->num_outqs;
	}

	info->max_mac_addrs = 0;

	/* Assumes all queues has the same value */
	info->rx_desc_lim.nb_max =
		priv->gpio_capa.intcs_inf.intcs_inf[0].inqs_inf[0].size + 1;
	info->rx_desc_lim.nb_min = info->rx_desc_lim.nb_max;
	info->rx_desc_lim.nb_align = MVGIU_RXD_ALIGN;
	info->rx_desc_lim.nb_seg_max = 1;
	if (priv->gpio_capa.sg_en)
		info->rx_desc_lim.nb_seg_max = GIU_GPIO_MAX_SG_SEGMENTS;
	info->rx_desc_lim.nb_mtu_seg_max = info->rx_desc_lim.nb_seg_max;

	/* Assumes all queues has the same value */
	info->tx_desc_lim.nb_max =
		priv->gpio_capa.outtcs_inf.outtcs_inf[0].outqs_inf[0].size + 1;
	info->tx_desc_lim.nb_min = info->tx_desc_lim.nb_max;
	info->tx_desc_lim.nb_align = MVGIU_TXD_ALIGN;
	info->tx_desc_lim.nb_seg_max = 1;
	if (priv->gpio_capa.sg_en)
		info->tx_desc_lim.nb_seg_max = GIU_GPIO_MAX_SG_SEGMENTS;
	info->tx_desc_lim.nb_mtu_seg_max = info->tx_desc_lim.nb_seg_max;

	info->rx_offload_capa = MVGIU_RX_OFFLOADS;
	info->rx_offload_capa |= DEV_RX_OFFLOAD_JUMBO_FRAME;
	if (priv->gpio_capa.sg_en)
		info->rx_offload_capa |= DEV_RX_OFFLOAD_SCATTER;
	info->rx_queue_offload_capa = info->rx_offload_capa;

	info->tx_offload_capa = MVGIU_TX_OFFLOADS;
	if (priv->gpio_capa.sg_en)
		info->tx_offload_capa |= DEV_TX_OFFLOAD_MULTI_SEGS;
	info->tx_queue_offload_capa = info->tx_offload_capa;

	info->flow_type_rss_offloads = 0;

	/* By default packets are dropped if no descriptors are available */
	info->default_rxconf.rx_drop_en = 1;

	/* one buffer size */
	info->max_rx_pktlen = priv->bpool_capa.buff_len;
	if (priv->gpio_capa.sg_en) {
		/* maximum number of s/g entries multiple by buf-size */
		info->max_rx_pktlen =
			GIU_GPIO_MAX_SG_SEGMENTS * priv->bpool_capa.buff_len;
	}
	info->max_mtu = info->max_rx_pktlen;

	/* mempool buffer size MUST be at least as the giu bpool buffer size */
	info->min_rx_bufsize = priv->bpool_capa.buff_len;

	return 0;
}

/**
 * DPDK callback to get information about specific receive queue.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param rx_queue_id
 *   Receive queue index.
 * @param qinfo
 *   Receive queue information structure.
 */
static void mvgiu_rxq_info_get(struct rte_eth_dev *dev, uint16_t rx_queue_id,
			      struct rte_eth_rxq_info *qinfo)
{
	struct mvgiu_rxq *rxq = dev->data->rx_queues[rx_queue_id];

	qinfo->mp = rxq->mp;
	qinfo->nb_desc = rxq->size;
}

/**
 * DPDK callback to get information about specific transmit queue.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param tx_queue_id
 *   Transmit queue index.
 * @param qinfo
 *   Transmit queue information structure.
 */
static void mvgiu_txq_info_get(struct rte_eth_dev *dev, uint16_t tx_queue_id,
			      struct rte_eth_txq_info *qinfo)
{
	struct mvgiu_txq *txq = dev->data->tx_queues[tx_queue_id];

	qinfo->nb_desc = txq->size;
	qinfo->conf.tx_deferred_start = txq->tx_deferred_start;
}

/**
 * DPDK callback to configure the receive queue.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param idx
 *   RX queue index.
 * @param desc
 *   Number of descriptors to configure in queue.
 * @param socket
 *   NUMA socket on which memory must be allocated.
 * @param conf
 *   Thresholds parameters.
 * @param mp
 *   Memory pool for buffer allocations.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_rx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
		    unsigned int socket,
		    const struct rte_eth_rxconf *conf,
		    struct rte_mempool *mp)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	struct mvgiu_rxq *rxq;
	uint32_t min_size;
	uint32_t max_rx_pkt_len = dev->data->dev_conf.rxmode.max_rx_pkt_len;
	uint64_t offloads;
	int ret;

	offloads = conf->offloads | dev->data->dev_conf.rxmode.offloads;

	min_size = rte_pktmbuf_data_room_size(mp) - RTE_PKTMBUF_HEADROOM -
		   MVGIU_PKT_EFFEC_OFFS;
	if (!(offloads & DEV_RX_OFFLOAD_SCATTER) &&
	     (min_size < max_rx_pkt_len)) {
		RTE_LOG(ERR, PMD,
			"Mbuf size must be increased to %u bytes to hold up to %u bytes of data.\n",
			max_rx_pkt_len + RTE_PKTMBUF_HEADROOM +
			MVGIU_PKT_EFFEC_OFFS,
			max_rx_pkt_len);
		return -EINVAL;
	}

	if (dev->data->rx_queues[idx]) {
		rte_free(dev->data->rx_queues[idx]);
		dev->data->rx_queues[idx] = NULL;
	}

	rxq = rte_zmalloc_socket("rxq", sizeof(*rxq), 0, socket);
	if (!rxq)
		return -ENOMEM;

	rxq->priv = priv;
	rxq->mp = mp;
	rxq->queue_id = idx;
	rxq->port_id = dev->data->port_id;
	rxq->size = desc;
	rxq->cksum_enabled = offloads & DEV_RX_OFFLOAD_CHECKSUM;

	if (priv->bpool_init_size < priv->bpool_capa.max_num_buffs) {
		/* more buffers can be filled */
		if ((priv->bpool_init_size + desc) >=
		    priv->bpool_capa.max_num_buffs)
			desc = priv->bpool_capa.max_num_buffs -
				priv->bpool_init_size;

		ret = mvgiu_fill_bpool(rxq, desc);
		if (ret) {
			rte_free(rxq);
			return ret;
		}

		priv->bpool_init_size += desc;
	}
	dev->data->rx_queues[idx] = rxq;

	return 0;
}

/**
 * DPDK callback to release the receive queue.
 *
 * @param rxq
 *   Generic receive queue pointer.
 */
static void
mvgiu_rx_queue_release(void *rxq)
{
	struct mvgiu_rxq *q = rxq;

	mvgiu_drain_bpool(q->priv, q->size);
	rte_free(q);
}

/**
 * DPDK callback to configure the transmit queue.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param idx
 *   Transmit queue index.
 * @param desc
 *   Number of descriptors to configure in the queue.
 * @param socket
 *   NUMA socket on which memory must be allocated.
 * @param conf
 *   Tx queue configuration parameters.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_tx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
		    unsigned int socket,
		    const struct rte_eth_txconf *conf)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	struct mvgiu_txq *txq;

	if (dev->data->tx_queues[idx]) {
		rte_free(dev->data->tx_queues[idx]);
		dev->data->tx_queues[idx] = NULL;
	}

	txq = rte_zmalloc_socket("txq", sizeof(*txq), 0, socket);
	if (!txq)
		return -ENOMEM;

	txq->priv = priv;
	txq->queue_id = idx;
	txq->port_id = dev->data->port_id;
	txq->tx_deferred_start = conf->tx_deferred_start;
	txq->size = desc;

	txq->shadow_txq.total_size = txq->size;
	txq->shadow_txq.ent =
		rte_zmalloc_socket("txq-shadow-ents",
				   sizeof(struct giu_buff_inf) *
					txq->shadow_txq.total_size,
				   0,
				   socket);
	if (!txq->shadow_txq.ent)
		return -ENOMEM;
	txq->shadow_txq.descs =
		rte_zmalloc_socket("txq-shadow-descs",
				    sizeof(struct giu_gpio_desc) *
					txq->size,
				    0,
				    socket);
	if (!txq->shadow_txq.descs)
		return -ENOMEM;

	dev->data->tx_queues[idx] = txq;

	return 0;
}

/**
 * DPDK callback to release the transmit queue.
 *
 * @param txq
 *   Generic transmit queue pointer.
 */
static void
mvgiu_tx_queue_release(void *txq)
{
	struct mvgiu_txq *q = txq;

	if (!q)
		return;

	rte_free(q);
}

/**
 * DPDK callback to get device statistics.
 *
 * @param dev
 *   Pointer to Ethernet device structure.
 * @param stats
 *   Stats structure output buffer.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_stats_get(struct rte_eth_dev *dev, struct rte_eth_stats *stats)
{
	struct mvgiu_priv *priv = dev->data->dev_private;
	unsigned int i, idx;

	if (!priv->gpio)
		return -EPERM;

	for (i = 0; i < dev->data->nb_rx_queues; i++) {
		struct mvgiu_rxq *rxq = dev->data->rx_queues[i];

		if (!rxq)
			continue;

		idx = rxq->queue_id;
		if (unlikely(idx >= RTE_ETHDEV_QUEUE_STAT_CNTRS)) {
			RTE_LOG(ERR, PMD,
				"rx queue %d stats out of range (0 - %d)\n",
				idx, RTE_ETHDEV_QUEUE_STAT_CNTRS - 1);
			continue;
		}

		stats->q_ibytes[idx] = rxq->bytes_recv;
		stats->q_ipackets[idx] = rxq->packets_recv;
		stats->q_errors[idx] = 0;
		stats->ibytes += stats->q_ibytes[idx];
		stats->ipackets += stats->q_ipackets[idx];
	}

	for (i = 0; i < dev->data->nb_tx_queues; i++) {
		struct mvgiu_txq *txq = dev->data->tx_queues[i];

		if (!txq)
			continue;

		idx = txq->queue_id;
		if (unlikely(idx >= RTE_ETHDEV_QUEUE_STAT_CNTRS)) {
			RTE_LOG(ERR, PMD,
				"tx queue %d stats out of range (0 - %d)\n",
				idx, RTE_ETHDEV_QUEUE_STAT_CNTRS - 1);
		}

		stats->q_obytes[idx] = txq->bytes_sent;
		stats->q_opackets[idx] = txq->packets_sent;
		stats->obytes += stats->q_obytes[idx];
		stats->opackets += stats->q_opackets[idx];
	}

	stats->imissed += 0;
	stats->ierrors += 0;
	stats->rx_nombuf += 0;

	return 0;
}

static const struct eth_dev_ops mvgiu_ops = {
	.dev_configure = mvgiu_dev_configure,
	.dev_start = mvgiu_dev_start,
	.dev_stop = mvgiu_dev_stop,
	.dev_set_link_up = mvgiu_dev_set_link_up,
	.dev_set_link_down = mvgiu_dev_set_link_down,
	.dev_close = mvgiu_dev_close,
	.link_update = mvgiu_link_update,
	.promiscuous_enable = NULL,
	.allmulticast_enable = NULL,
	.promiscuous_disable = NULL,
	.allmulticast_disable = NULL,
	.mac_addr_remove = NULL,
	.mac_addr_add = NULL,
	.mac_addr_set = NULL,
	.mtu_set = NULL,
	.stats_get = mvgiu_stats_get,
	.stats_reset = NULL,
	.xstats_get = NULL,
	.xstats_reset = NULL,
	.xstats_get_names = NULL,
	.dev_infos_get = mvgiu_dev_infos_get,
	.dev_supported_ptypes_get = NULL,
	.rxq_info_get = mvgiu_rxq_info_get,
	.txq_info_get = mvgiu_txq_info_get,
	.vlan_filter_set = NULL,
	.tx_queue_start = NULL,
	.tx_queue_stop = NULL,
	.rx_queue_setup = mvgiu_rx_queue_setup,
	.rx_queue_release = mvgiu_rx_queue_release,
	.tx_queue_setup = mvgiu_tx_queue_setup,
	.tx_queue_release = mvgiu_tx_queue_release,
	.flow_ctrl_get = NULL,
	.flow_ctrl_set = NULL,
	.rss_hash_update = NULL,
	.rss_hash_conf_get = NULL,
	.filter_ctrl = NULL,
	.xstats_get_by_id = NULL,
	.xstats_get_names_by_id = NULL
};

static inline uint64_t
mvgiu_desc_to_packet_type_and_offset(struct giu_gpio_desc *desc,
				     uint8_t *l3_offset,
				     uint8_t *l4_offset,
				     int *gen_l3_chk,
				     int *gen_l4_chk)
{
	enum giu_inq_l3_type l3_type;
	enum giu_inq_l4_type l4_type;
	enum giu_vlan_tag vlan_tag;
	uint64_t packet_type;

	giu_gpio_inq_desc_get_l2_info(desc, &vlan_tag);
	giu_gpio_inq_desc_get_l3_info(desc, &l3_type, l3_offset, gen_l3_chk);
	giu_gpio_inq_desc_get_l4_info(desc, &l4_type, l4_offset, gen_l4_chk);

	packet_type = RTE_PTYPE_L2_ETHER;

	switch (vlan_tag) {
	case GIU_VLAN_TAG_SINGLE:
		packet_type |= RTE_PTYPE_L2_ETHER_VLAN;
		break;
	case GIU_VLAN_TAG_DOUBLE:
		packet_type |= RTE_PTYPE_L2_ETHER_QINQ;
		break;
	default:
		break;
	}

	switch (l3_type) {
	case GIU_INQ_L3_TYPE_IPV4:
		packet_type |= RTE_PTYPE_L3_IPV4;
		break;
	case GIU_INQ_L3_TYPE_IPV6:
		packet_type |= RTE_PTYPE_L3_IPV6;
		break;
	default:
		break;
	}

	switch (l4_type) {
	case GIU_INQ_L4_TYPE_TCP:
		packet_type |= RTE_PTYPE_L4_TCP;
		break;
	case GIU_INQ_L4_TYPE_UDP:
		packet_type |= RTE_PTYPE_L4_UDP;
		break;
	default:
		break;
	}

	return packet_type;
}

static inline uint64_t
mvgiu_desc_to_ol_flags(int gen_l3_chk, int gen_l4_chk, uint64_t packet_type)
{
	uint64_t flags = 0;

	if (RTE_ETH_IS_IPV4_HDR(packet_type)) {
		if (gen_l3_chk)
			flags |= PKT_RX_IP_CKSUM_UNKNOWN;
		else
			flags |= PKT_RX_IP_CKSUM_GOOD;
	}

	if (((packet_type & RTE_PTYPE_L4_UDP) == RTE_PTYPE_L4_UDP) ||
	    ((packet_type & RTE_PTYPE_L4_TCP) == RTE_PTYPE_L4_TCP)) {
		if (gen_l4_chk)
			flags |= PKT_RX_L4_CKSUM_UNKNOWN;
		else
			flags |= PKT_RX_L4_CKSUM_GOOD;
	}

	return flags;
}

/**
 * Set S/G mbuf pkt length.
 *
 * @param[in] desc
 *	A pointer to the S/G first segment.
 * @param[in] idx
 *	The first S/G segment desc index in the descs array.
 * @param[in] num_sg_entries
 *	Number of S/G entries in the jambo packet.
 */
static inline void
mvgiu_rx_sg_mbuf_set(struct giu_gpio_desc *descs,
			  uint8_t num_sg_entries)
{
	struct rte_mbuf *first_seg = NULL, *prev_seg = NULL, *cur_seg = NULL;
	uint8_t i = 0;

	first_seg =
		(struct rte_mbuf *)(giu_gpio_inq_desc_get_cookie(&descs[i]));
	first_seg->pkt_len = giu_gpio_inq_desc_get_pkt_len(&descs[i]);
	first_seg->data_len = first_seg->pkt_len;
	rte_mbuf_refcnt_set(first_seg, 1);
	first_seg->nb_segs = 1;
	prev_seg = first_seg;
	while (--num_sg_entries) {
		i++;
		cur_seg =
		(struct rte_mbuf *)(giu_gpio_inq_desc_get_cookie(&descs[i]));
		rte_pktmbuf_reset(cur_seg);
		cur_seg->data_len = giu_gpio_inq_desc_get_pkt_len(&descs[i]);
		first_seg->pkt_len += cur_seg->data_len;
		rte_mbuf_refcnt_set(cur_seg, 1);
		first_seg->nb_segs++;
		prev_seg->next = cur_seg;
		prev_seg = cur_seg;
	}

	if (cur_seg != NULL)
		cur_seg->next = NULL;
}

/**
 * S/G desc handling
 *
 * Validate that all the S/G enries are in the rxq, set the S/G info to the mbuf
 *
 * @param[in] descs
 *	A pointer to a packet descriptor structure array.
 * @param[in] idx
 *	A pointer to the current desc in the descs array, will be used for
 *	verified that all the S/G entries already exsist in the descs array.
 * @param[in] nb_descs
 *	The numbers of descs that currently handled by the Rx burst func.
 * @param[out] idx
 *	A pointer to the last s/g desc in the descs array.
 *
 * @Return
 *	Status 0 on success.
 */
static inline int
mvgiu_rx_sg_handling(struct giu_gpio_desc *descs, int *idx,
			   struct mvgiu_rxq *sw_rxq, uint16_t nb_descs)
{
	uint16_t num_sg_entries, num_descs, new_desc_idx;
	int additional_descs;
	struct mvgiu_priv *priv = sw_rxq->priv;
	struct giu_gpio_desc new_descs[GIU_GPIO_MAX_SG_SEGMENTS];

	num_sg_entries = giu_gpio_inq_desc_get_num_sg_ent(&descs[*idx]);

	/* Validate that all the s/g enties are in the descs queue */

	additional_descs = num_sg_entries - (nb_descs - *idx);

	if (additional_descs > 0) {
		num_descs = additional_descs;

		/* Copy the old s/g descs to the new descs array */
		memcpy(&new_descs[0], &descs[*idx],
		       (num_sg_entries - additional_descs) * sizeof(*descs));

		/* new_desc_idx it's the idx of the first new desc in the new
		 * descs array that stored the the old and new descs.
		 */
		new_desc_idx = num_sg_entries - additional_descs;

		/* Get additional descs from the hw rxq */
		giu_gpio_recv(priv->gpio, priv->rxq_map[sw_rxq->queue_id].tc,
			      priv->rxq_map[sw_rxq->queue_id].inq,
			      &new_descs[new_desc_idx], &num_descs);

		if (num_descs == additional_descs) {
			/* Set S/G mbufs infos */
			mvgiu_rx_sg_mbuf_set(&new_descs[0],
						  num_sg_entries);
			/*
			 * Descs index update, the return idx in this case equal
			 * to the count of the old descs array minus one.
			 */
			*idx = nb_descs - 1;
			return 0;
		}
		/* If part of the s/g entries are missing in the rxq */
		uint16_t i, actual_sg_descs;
		struct rte_mbuf *mbuf;

		actual_sg_descs = num_sg_entries - additional_descs +
				  num_descs;
		RTE_LOG(ERR, PMD, "Not all the S/G descs exist in the HW RXQ, "
			"expected: %d descs, actual: %d descs\n",
			num_sg_entries, actual_sg_descs);
		for (i = 0; i < actual_sg_descs; i++) {
			mbuf = (struct rte_mbuf *)(cookie_addr_high |
				giu_gpio_inq_desc_get_cookie(&new_descs[i]));
			rte_pktmbuf_free(mbuf);
		}

		/*
		 * Descs index update, the return idx in this case equal
		 * to the size of the old descs array minus one
		 */
		*idx = nb_descs - 1;
		return -1;
	}
	/* Set S/G mbufs infos */
	mvgiu_rx_sg_mbuf_set(&descs[*idx], num_sg_entries);
	/*
	 * Descs index update, the return idx in this case equal to the index of
	 * the first s/g entrie plus the num of the total s/g entries minus one
	 */
	*idx = *idx + num_sg_entries - 1;
	return 0;
}

/**
 * DPDK callback for receive.
 *
 * @param rxq
 *   Generic pointer to the receive queue.
 * @param rx_pkts
 *   Array to store received packets.
 * @param nb_pkts
 *   Maximum number of packets in array.
 *
 * @return
 *   Number of packets successfully received.
 */
static uint16_t
mvgiu_rx_pkt_burst(void *rxq, struct rte_mbuf **rx_pkts, uint16_t nb_descs)
{
	struct mvgiu_rxq *q = rxq;
	struct giu_gpio_desc descs[nb_descs];
	struct giu_bpool *bpool;
	int i, ret = 0, rx_done = 0;
	uint8_t l3_offset, l4_offset;
	int gen_l3_chk, gen_l4_chk;

	bpool = q->priv->bpool;

	ret = giu_gpio_recv(q->priv->gpio,
			    q->priv->rxq_map[q->queue_id].tc,
			    q->priv->rxq_map[q->queue_id].inq,
			    descs,
			    &nb_descs);
	if (unlikely(ret < 0)) {
		RTE_LOG(ERR, PMD, "Failed to receive packets\n");
		return 0;
	}

	for (i = 0; i < nb_descs; i++) {
		struct rte_mbuf *mbuf;
		uint64_t addr;

		if (likely(nb_descs - i > MRVL_MUSDK_PREFETCH_SHIFT)) {
			struct giu_gpio_desc *pref_desc;
			u64 pref_addr;

			pref_desc = &descs[i + MRVL_MUSDK_PREFETCH_SHIFT];
			pref_addr = cookie_addr_high |
				    giu_gpio_inq_desc_get_cookie(pref_desc);
			rte_mbuf_prefetch_part1((struct rte_mbuf *)(pref_addr));
			rte_mbuf_prefetch_part2((struct rte_mbuf *)(pref_addr));
		}

		addr = giu_gpio_inq_desc_get_cookie(&descs[i]);
		mbuf = (struct rte_mbuf *)(cookie_addr_high | addr);
		rte_pktmbuf_reset(mbuf);

		mbuf->data_off += q->data_offset;
		mbuf->pkt_len = giu_gpio_inq_desc_get_pkt_len(&descs[i]);
		mbuf->data_len = mbuf->pkt_len;
		mbuf->port = q->port_id;

		mbuf->packet_type =
			mvgiu_desc_to_packet_type_and_offset(&descs[i],
							     &l3_offset,
							     &l4_offset,
							     &gen_l3_chk,
							     &gen_l4_chk);
		mbuf->l2_len = l3_offset;
		mbuf->l3_len = l4_offset - l3_offset;

		if (likely(q->cksum_enabled))
			mbuf->ol_flags =
				mvgiu_desc_to_ol_flags(gen_l3_chk,
						       gen_l4_chk,
						       mbuf->packet_type);
		if (GIU_FORMAT_DIRECT_SG ==
			giu_gpio_inq_desc_get_format(&descs[i])) {
			if (mvgiu_rx_sg_handling(descs, &i, q, nb_descs)) {
				/* Missing s/g entries in the RXQ case,
				 * the used mbufs already freed in
				 * mvgiu_rx_sg_handling func
				 */
				continue;
			}
		}

		rx_pkts[rx_done++] = mbuf;
		q->bytes_recv += mbuf->pkt_len;
	}

	if (rte_spinlock_trylock(&q->priv->lock) == 1) {
		uint32_t num;

		giu_bpool_get_num_buffs(bpool, &num);

		if (unlikely(num <= (uint32_t)(q->priv->bpool_init_size -
						MRVL_BURST_SIZE))) {
			num = q->priv->bpool_init_size - num;
			ret = mvgiu_fill_bpool(q, num);
			if (ret)
				RTE_LOG(DEBUG, PMD,
					"Failed to fill bpool, num %d\n", num);
		}
		rte_spinlock_unlock(&q->priv->lock);
	}

	q->packets_recv += rx_done;

	return rx_done;
}

/**
 * Prepare parsing information.
 */
static inline void
mvgiu_prepare_proto_info(uint64_t ol_flags,
			 uint32_t packet_type,
			 enum giu_outq_l3_type *l3_type,
			 enum giu_outq_l4_type *l4_type,
			 enum giu_vlan_tag *vlan_tag,
			 enum giu_outq_ip_status *ip_status,
			 enum giu_outq_l4_status *l4_status)
{
	if ((ol_flags & PKT_TX_IPV4) || RTE_ETH_IS_IPV4_HDR(packet_type))
		*l3_type = GIU_OUTQ_L3_TYPE_IPV4_NO_OPTS;
	else if ((ol_flags & PKT_TX_IPV6) || RTE_ETH_IS_IPV6_HDR(packet_type))
		*l3_type = GIU_OUTQ_L3_TYPE_IPV6_NO_EXT;
	else if (packet_type & RTE_PTYPE_L2_ETHER_ARP)
		*l3_type = RTE_PTYPE_L2_ETHER_ARP;
	else
		*l3_type = GIU_OUTQ_L3_TYPE_NA;

	if (packet_type & RTE_PTYPE_L4_TCP)
		*l4_type = GIU_OUTQ_L4_TYPE_TCP;
	else if (packet_type & RTE_PTYPE_L4_UDP)
		*l4_type = GIU_OUTQ_L4_TYPE_UDP;
	else if (packet_type & RTE_PTYPE_L4_MASK)
		*l4_type = GIU_OUTQ_L4_TYPE_OTHER;
	else
		*l4_type = GIU_OUTQ_L4_TYPE_NA;

	if (packet_type & RTE_PTYPE_L2_ETHER_VLAN)
		*vlan_tag = GIU_VLAN_TAG_SINGLE;
	else if (packet_type & RTE_PTYPE_L2_ETHER_QINQ)
		*vlan_tag = GIU_VLAN_TAG_DOUBLE;
	else
		*vlan_tag = GIU_VLAN_TAG_NONE;

	if ((ol_flags & PKT_RX_IP_CKSUM_GOOD) == PKT_RX_IP_CKSUM_GOOD)
		*ip_status = GIU_OUTQ_IP_OK;
	else if ((ol_flags & PKT_RX_IP_CKSUM_BAD) == PKT_RX_IP_CKSUM_BAD)
		*ip_status = GIU_OUTQ_IPV4_CHECKSUM_ERR;
	else
		*ip_status = GIU_OUTQ_IPV4_CHECKSUM_UNKNOWN;

	if ((ol_flags & PKT_RX_L4_CKSUM_GOOD) == PKT_RX_L4_CKSUM_GOOD)
		*l4_status = GIU_OUTQ_L4_OK;
	else if ((ol_flags & PKT_RX_L4_CKSUM_BAD) == PKT_RX_L4_CKSUM_BAD)
		*l4_status = GIU_OUTQ_L4_CHECKSUM_ERR;
	else
		*l4_status = GIU_OUTQ_L4_CHECKSUM_UNKNOWN;
}

/**
 * DPDK callback for transmit.
 *
 * @param txq
 *   Generic pointer transmit queue.
 * @param tx_pkts
 *   Packets to transmit.
 * @param nb_pkts
 *   Number of packets in array.
 *
 * @return
 *   Number of packets successfully transmitted.
 */
static uint16_t
mvgiu_tx_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
{
	struct mvgiu_txq *q = txq;
	struct mvgiu_shadow_txq *sq;
	struct giu_gpio_desc descs[nb_pkts];
	uint16_t i, sq_free_size;
	uint64_t bytes_sent = 0;
	uint8_t tc = 0;

	sq = &q->shadow_txq;

	if (unlikely(!q->priv->gpio))
		return 0;

	if (sq->size)
		mvgiu_check_n_free_sent_buffers(q->priv->gpio,
						sq,
						tc,
						q->queue_id);
	/*
	 * Reduce the num of pkt that will be transmitted in case of limited
	 * free desc in the shadow queue which rerepsent the hw-queue
	 */
	sq_free_size = sq->total_size - sq->size - 1;
	if (unlikely(nb_pkts > sq_free_size)) {
		RTE_LOG(DEBUG, PMD,
			"No room in shadow queue for %d packets! %d packets will be sent.\n",
			nb_pkts, sq_free_size);
		nb_pkts = sq_free_size;
	}

	for (i = 0; i < nb_pkts; i++) {
		struct rte_mbuf *mbuf = tx_pkts[i];
		enum giu_outq_l3_type l3_type;
		enum giu_outq_l4_type l4_type;
		enum giu_vlan_tag vlan_tag;
		enum giu_outq_ip_status ip_status;
		enum giu_outq_l4_status l4_status;

		if (likely(nb_pkts - i > MRVL_MUSDK_PREFETCH_SHIFT)) {
			struct rte_mbuf *pref_pkt_hdr;

			pref_pkt_hdr = tx_pkts[i + MRVL_MUSDK_PREFETCH_SHIFT];
			rte_mbuf_prefetch_part1(pref_pkt_hdr);
			rte_mbuf_prefetch_part2(pref_pkt_hdr);
		}

		sq->ent[sq->head].cookie = (uint64_t)mbuf;
		sq->ent[sq->head].addr =
			rte_mbuf_data_iova_default(mbuf);
		sq->head = (sq->head + 1) & (sq->total_size - 1);
		sq->size++;

		giu_gpio_outq_desc_reset(&descs[i]);
		giu_gpio_outq_desc_set_phys_addr(&descs[i],
						 rte_pktmbuf_iova(mbuf));
		giu_gpio_outq_desc_set_pkt_offset(&descs[i], 0);
		giu_gpio_outq_desc_set_pkt_len(&descs[i],
					       rte_pktmbuf_pkt_len(mbuf));

		bytes_sent += rte_pktmbuf_pkt_len(mbuf);

		mvgiu_prepare_proto_info(mbuf->ol_flags,
					 mbuf->packet_type,
					 &l3_type,
					 &l4_type,
					 &vlan_tag,
					 &ip_status,
					 &l4_status);

		giu_gpio_outq_desc_set_virt_addr(&descs[i],
						 rte_pktmbuf_mtod(mbuf,
								  void *));
		giu_gpio_outq_desc_set_proto_info(&descs[i],
						  ip_status,
						  l4_status,
						  GIU_OUTQ_L2_UNICAST,
						  vlan_tag,
						  GIU_OUTQ_L3_UNICAST,
						  l3_type,
						  mbuf->l2_len,
						  l4_type,
						  mbuf->l2_len + mbuf->l3_len);

		if (mbuf->ol_flags & PKT_RX_RSS_HASH) {
			giu_gpio_outq_desc_set_hk_mode(&descs[i],
						       mbuf->hash.rss);
		}
	}

	giu_gpio_send(q->priv->gpio, tc, q->queue_id, descs, &i);
	/*
	 * Since it was already checked that there is free room in the shadow q
	 * for all the descs, no need to verify that the requested nb_descs were
	 * actually sent.
	 */

	q->bytes_sent += bytes_sent;
	q->packets_sent += i;

	return i;
}

/**
 * Scatter/Gather Tx handling
 * @param[in] desc
 *	pointer to the s/g first target tx descriptor.
 * @param[in] mbuf
 *	A Pointer to the first s/g mbuf.
 *
 */
static inline void
mvgiu_tx_sg_handling(struct giu_gpio_desc *descs, struct rte_mbuf *mbuf)
{
	struct rte_mbuf *cur_seg = mbuf;
	uint16_t i = 0;

	/* First s/g segment setting */
	giu_gpio_outq_desc_set_format(&descs[i], GIU_FORMAT_DIRECT_SG,
				      cur_seg->nb_segs);

	giu_gpio_outq_desc_set_pkt_len(&descs[i],
					rte_pktmbuf_data_len(cur_seg));
	/* Rest of segments setting */
	while (cur_seg->next != NULL) {
		i++;
		cur_seg = cur_seg->next;
		giu_gpio_outq_desc_reset(&descs[i]);
		giu_gpio_outq_desc_set_format(&descs[i],
					       GIU_FORMAT_NONE, 0);
		/* Set the physical address in an outq packet descriptor */
		giu_gpio_outq_desc_set_phys_addr(&descs[i],
						  rte_pktmbuf_iova(cur_seg));
		/* Set the packet offset in an outq packet descriptor */
		giu_gpio_outq_desc_set_pkt_offset(&descs[i], 0);

		giu_gpio_outq_desc_set_pkt_len(&descs[i],
					rte_pktmbuf_data_len(cur_seg));
	}
}

/**
 * DPDK callback for scatter/gather transmit.
 *
 * @param txq
 *   Generic pointer transmit queue.
 * @param tx_pkts
 *   Packets to transmit.
 * @param nb_pkts
 *   Number of packets in array.
 *
 * @return
 *   Number of packets successfully transmitted.
 */
static uint16_t
mvgiu_tx_sg_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
{
	struct mvgiu_txq *q = txq;
	struct mvgiu_shadow_txq *sq;
	struct giu_gpio_desc *descs;
	int i, bytes_sent = 0;
	uint16_t sq_free_size, nb_descs = 0;
	uint8_t tc = 0;

	sq = &q->shadow_txq;
	descs = sq->descs;

	if (unlikely(!q->priv->gpio))
		return 0;

	if (sq->size)
		mvgiu_check_n_free_sent_buffers(q->priv->gpio,
						sq,
						tc,
						q->queue_id);

	sq_free_size = sq->total_size - sq->size - 1;

	for (i = 0; (i < nb_pkts) && sq_free_size ; i++) {
		struct rte_mbuf *mbuf;
		enum giu_outq_l3_type l3_type;
		enum giu_outq_l4_type l4_type;
		enum giu_vlan_tag vlan_tag;
		enum giu_outq_ip_status ip_status;
		enum giu_outq_l4_status l4_status;

		if (likely(nb_pkts - i > MRVL_MUSDK_PREFETCH_SHIFT)) {
			struct rte_mbuf *pref_pkt_hdr;

			pref_pkt_hdr = tx_pkts[i + MRVL_MUSDK_PREFETCH_SHIFT];
			rte_mbuf_prefetch_part1(pref_pkt_hdr);
			rte_mbuf_prefetch_part2(pref_pkt_hdr);
		}
		mbuf = tx_pkts[i];

		giu_gpio_outq_desc_reset(&descs[nb_descs]);
		giu_gpio_outq_desc_set_phys_addr(&descs[nb_descs],
						 rte_pktmbuf_iova(mbuf));
		giu_gpio_outq_desc_set_pkt_offset(&descs[nb_descs], 0);
		giu_gpio_outq_desc_set_pkt_len(&descs[nb_descs],
					       rte_pktmbuf_pkt_len(mbuf));

		mvgiu_prepare_proto_info(mbuf->ol_flags,
					 mbuf->packet_type,
					 &l3_type,
					 &l4_type,
					 &vlan_tag,
					 &ip_status,
					 &l4_status);

		giu_gpio_outq_desc_set_virt_addr(&descs[nb_descs],
						 rte_pktmbuf_mtod(mbuf,
								  void *));
		giu_gpio_outq_desc_set_proto_info(&descs[nb_descs],
						  ip_status,
						  l4_status,
						  GIU_OUTQ_L2_UNICAST,
						  vlan_tag,
						  GIU_OUTQ_L3_UNICAST,
						  l3_type,
						  mbuf->l2_len,
						  l4_type,
						  mbuf->l2_len + mbuf->l3_len);

		if (mbuf->ol_flags & PKT_RX_RSS_HASH) {
			giu_gpio_outq_desc_set_hk_mode(&descs[nb_descs],
						       mbuf->hash.rss);
		}
		if (mbuf->nb_segs > 1) {
			if (mbuf->nb_segs > GIU_GPIO_MAX_SG_SEGMENTS) {
				RTE_LOG(ERR, PMD, "Pkt with more than %d "
					"scatter/gather segments not supported "
					"and will not be sent\n",
					GIU_GPIO_MAX_SG_SEGMENTS);
				goto pkts_send;
			}

			/*
			 * Check if there is free descs in the shedow queue for
			 * all the s/g segments.
			 */
			if (mbuf->nb_segs > sq_free_size) {
				RTE_LOG(DEBUG, PMD,  "No room in hw shadow "
					"queue for %d s/g segments, the pkt "
					"will not be sent\n", mbuf->nb_segs -
					sq_free_size);
				goto pkts_send;
			}

			mvgiu_tx_sg_handling(&descs[nb_descs], mbuf);
		}

		bytes_sent += rte_pktmbuf_pkt_len(mbuf);

		sq->ent[sq->head].cookie = (uint64_t)mbuf;
		sq->ent[sq->head].addr =
			rte_mbuf_data_iova_default(mbuf);
		sq->head = (sq->head + mbuf->nb_segs) & (sq->total_size - 1);
		sq->size += mbuf->nb_segs;
		sq_free_size -= mbuf->nb_segs;
		nb_descs += mbuf->nb_segs;
	}

pkts_send:
	giu_gpio_send(q->priv->gpio, tc, q->queue_id, descs, &nb_descs);

	/*
	 * Since it was already checked that there is headroom in the shadow q
	 * for all the descs, no need to verify that the requested nb_descs were
	 * actually coppied.
	 */

	q->bytes_sent += bytes_sent;
	q->packets_sent += i;

	return i;
}


/**
 * Create private device structure.
 *
 * @param dev_name
 *   Pointer to the port name passed in the initialization parameters.
 *
 * @return
 *   Pointer to the newly allocated private device structure.
 */
static struct mvgiu_priv *
mvgiu_priv_create(const char *dev_name)
{
	struct mvgiu_priv *priv;
	struct nmp_guest_info guest_inf;
	char	gpio_name[20], tmp_dev_name[15];
	struct nmp_guest_port_info *giu_info = NULL;
	char	*guest_prb_str, *gpio_id;
	int	ret, i;

	priv = rte_zmalloc_socket(dev_name, sizeof(*priv), 0, rte_socket_id());
	if (!priv)
		return NULL;

	if (!strstr(dev_name, "giu-")) {
		RTE_LOG(ERR, PMD, "device name should start with 'giu-'\n");
		goto out_free_priv;
	}
	strcpy(tmp_dev_name, dev_name);
	gpio_id = strtok(tmp_dev_name, "-");
	gpio_id = strtok(NULL, "-");
	memset(gpio_name, 0, sizeof(gpio_name));
	snprintf(gpio_name, sizeof(gpio_name), "gpio-0:%s", gpio_id);

	ret = rte_mvep_get_nmp_guest_info(&guest_inf, &guest_prb_str);
	if (ret)
		goto out_free_priv;

	for (i = 0; i < guest_inf.num_giu_ports; i++)
		if (strcmp(guest_inf.giu_info[i].port_name, gpio_name) == 0)
			break;
	if (i == guest_inf.num_giu_ports) {
		RTE_LOG(ERR, PMD,
			"gpio name %s not found in guest files\n", gpio_name);
		goto out_free_priv;
	}

	giu_info = &guest_inf.giu_info[i];
	if (giu_info->num_bpools != 1) {
		RTE_LOG(ERR, PMD, "only 1 giu-local-bpool is supported!!\n");
		goto out_free_priv;
	}

	/* Probe the GIU (local) BPool */
	ret = giu_bpool_probe(giu_info->bpool_info[0].bpool_name,
			guest_prb_str, &priv->bpool);
	if (ret) {
		RTE_LOG(ERR, PMD, "giu_bpool_probe failed!\n");
		goto out_free_priv;
	}

	ret = giu_bpool_get_capabilities(priv->bpool, &priv->bpool_capa);
	if (ret) {
		RTE_LOG(ERR, PMD, "giu_bpool_get_capabilities failed!\n");
		goto out_free_priv;
	}

	/* Probe the GIU GPIO */
	ret = giu_gpio_probe(giu_info->port_name,
				 guest_prb_str, &priv->gpio);
	if (ret) {
		RTE_LOG(ERR, PMD, "giu_gpio_probe failed!\n");
		goto out_free_priv;
	}

	ret = giu_gpio_get_capabilities(priv->gpio, &priv->gpio_capa);
	if (ret) {
		RTE_LOG(ERR, PMD, "giu_gpio_get_capabilities failed!\n");
		goto out_free_priv;
	}

	rte_spinlock_init(&priv->lock);

	return priv;
out_free_priv:
	rte_free(priv);
	return NULL;
}

/**
 * Create device representing Ethernet port.
 *
 * @param name
 *   Pointer to the port's name.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
mvgiu_eth_dev_create(struct rte_vdev_device *vdev, const char *name)
{
	struct rte_eth_dev *eth_dev;
	struct mvgiu_priv *priv;
	int ret;

	eth_dev = rte_eth_dev_allocate(name);
	if (!eth_dev) {
		RTE_LOG(ERR, PMD, "Failed to allocate GIU eth dev!\n");
		return -EFAULT;
	}

	priv = mvgiu_priv_create(name);
	if (!priv) {
		RTE_LOG(ERR, PMD, "Failed to create GIU!\n");
		ret = -ENOMEM;
		goto out_free_dev;
	}

	eth_dev->data->mac_addrs =
		rte_zmalloc("mac_addrs",
			    RTE_ETHER_ADDR_LEN * MVGIU_MAC_ADDRS_MAX, 0);
	if (!eth_dev->data->mac_addrs) {
		RTE_LOG(ERR, PMD, "Failed to allocate space for eth addrs\n");
		ret = -ENOMEM;
		goto out_free_priv;
	}

	eth_dev->data->kdrv = RTE_KDRV_NONE;
	eth_dev->data->dev_private = priv;
	eth_dev->device = &vdev->device;
	eth_dev->dev_ops = &mvgiu_ops;

	/* Flag to call rte_eth_dev_release_port() in rte_eth_dev_close(). */
	eth_dev->data->dev_flags |= RTE_ETH_DEV_CLOSE_REMOVE;

	rte_eth_dev_probing_finish(eth_dev);

	return 0;

out_free_priv:
	rte_free(priv);
out_free_dev:
	rte_eth_dev_release_port(eth_dev);

	return ret;
}

/**
 * Callback used by rte_kvargs_process() during argument parsing.
 *
 * @param key
 *   Pointer to the parsed key (unused).
 * @param value
 *   Pointer to the parsed value.
 * @param extra_args
 *   Pointer to the extra arguments which contains address of the
 *   table of pointers to parsed interface names.
 *
 * @return
 *   Always 0.
 */
static int
mvgiu_get_ifnames(const char *key __rte_unused, const char *value,
		 void *extra_args)
{
	struct mvgiu_ifnames *ifnames = extra_args;

	ifnames->names[ifnames->idx++] = value;

	return 0;
}

/**
 * DPDK callback to register the virtual device.
 *
 * @param vdev
 *   Pointer to the virtual device.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
rte_pmd_mvgiu_probe(struct rte_vdev_device *vdev)
{
	struct rte_kvargs *kvlist;
	struct mvgiu_ifnames ifnames;
	int ret = -EINVAL;
	uint32_t i, ifnum;
	const char *params;

	params = rte_vdev_device_args(vdev);
	if (!params)
		return -EINVAL;

	kvlist = rte_kvargs_parse(params, valid_args);
	if (!kvlist)
		return -EINVAL;

	ifnum = rte_kvargs_count(kvlist, MRVL_IFACE_NAME_ARG);
	if (ifnum > RTE_DIM(ifnames.names))
		goto out_free_kvlist;

	ifnames.idx = 0;
	rte_kvargs_process(kvlist, MRVL_IFACE_NAME_ARG,
			   mvgiu_get_ifnames, &ifnames);

	if (mrvl_dev_num)
		goto init_devices;

	ret = rte_mvep_init(MVEP_MOD_T_GIU, kvlist);
	if (ret)
		goto out_free_kvlist;

init_devices:
	for (i = 0; i < ifnum; i++) {
		RTE_LOG(INFO, PMD, "Creating %s\n", ifnames.names[i]);
		ret = mvgiu_eth_dev_create(vdev, ifnames.names[i]);
		if (ret)
			goto out_cleanup;
		mrvl_dev_num++;
	}

	rte_kvargs_free(kvlist);

	return 0;

out_cleanup:
	rte_pmd_mvgiu_remove(vdev);

out_free_kvlist:
	rte_kvargs_free(kvlist);

	return ret;
}

/**
 * DPDK callback to remove virtual device.
 *
 * @param vdev
 *   Pointer to the removed virtual device.
 *
 * @return
 *   0 on success, negative error value otherwise.
 */
static int
rte_pmd_mvgiu_remove(struct rte_vdev_device *vdev)
{
	uint16_t port_id;

	RTE_ETH_FOREACH_DEV(port_id) {
		if (rte_eth_devices[port_id].device != &vdev->device)
			continue;
		rte_eth_dev_close(port_id);
	}

	return 0;
}

static struct rte_vdev_driver pmd_mvgiu_drv = {
	.probe = rte_pmd_mvgiu_probe,
	.remove = rte_pmd_mvgiu_remove,
};

RTE_PMD_REGISTER_VDEV(net_mvgiu, pmd_mvgiu_drv);
RTE_PMD_REGISTER_ALIAS(net_mvgiu, eth_mvgiu);
